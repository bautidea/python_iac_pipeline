{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Gonna use postgres, since it runs on the same port and has a similar structure than Redshift.\n",
    "import psycopg2\n",
    "\n",
    "# Gonna use this library to read config file\n",
    "import configparser\n",
    "\n",
    "import json\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading config file.\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('../../AWS credetials/DataPipeline_python_IaC/cluster.config'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using config object to get config parameters.\n",
    "key = config.get('AWS', 'KEY')\n",
    "secret = config.get('AWS', 'SECRET')\n",
    "\n",
    "dwh_cluster_type = config.get('DWH', 'DWH_CLUSTER_TYPE')\n",
    "dwh_num_nodes = config.get('DWH', 'DWH_NUM_NODES')\n",
    "dwh_node_type = config.get('DWH', 'DWH_NODE_TYPE')\n",
    "dwh_cluster_identifier = config.get('DWH', 'DWH_CLUSTER_IDENTIFIER')\n",
    "dwh_db = config.get('DWH', 'DWH_DB')\n",
    "dwh_db_user = config.get('DWH', 'DWH_DB_USER')\n",
    "dwh_db_password = config.get('DWH', 'DWH_DB_PASSWORD')\n",
    "dwh_port = config.get('DWH', 'DWH_PORT')\n",
    "dwh_iam_role_name = config.get('DWH', 'DWH_IAM_ROLE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to diferent services using boto3.\n",
    "ec2 = boto3.resource('ec2', \n",
    "                     region_name='sa-east-1',\n",
    "                     aws_access_key_id=key,\n",
    "                     aws_secret_access_key = secret\n",
    "                     )\n",
    "\n",
    "s3 = boto3.resource('s3', \n",
    "                     region_name='sa-east-1',\n",
    "                     aws_access_key_id=key,\n",
    "                     aws_secret_access_key = secret\n",
    "                     )\n",
    "\n",
    "iam = boto3.client('iam', \n",
    "                     region_name='sa-east-1',\n",
    "                     aws_access_key_id=key,\n",
    "                     aws_secret_access_key = secret\n",
    "                     )\n",
    "\n",
    "redshift = boto3.client('redshift', \n",
    "                     region_name='sa-east-1',\n",
    "                     aws_access_key_id=key,\n",
    "                     aws_secret_access_key = secret\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allevents_pipe.txt',\n",
       " 'allusers_pipe.txt',\n",
       " 'category_pipe.txt',\n",
       " 'date2008_pipe.txt',\n",
       " 'listings_pipe.txt',\n",
       " 'sales_tab.txt',\n",
       " 'venue_pipe.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaning all my files available in S3.\n",
    "bucket = s3.Bucket('datapipeline-python-iac')\n",
    "\n",
    "log_data_files = [filename.key for filename in bucket.objects.all()]\n",
    "log_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Iam Role ARN, so i can pass Iam access to my cluster.\n",
    "role_arn = iam.get_role(RoleName=dwh_iam_role_name)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "# Creating my Redshift cluster.\n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        ClusterType = dwh_cluster_type,\n",
    "        NodeType = dwh_node_type,\n",
    "        \n",
    "        # Identifiers and credentials.\n",
    "        DBName = dwh_db,\n",
    "        ClusterIdentifier = dwh_cluster_identifier,\n",
    "        MasterUsername = dwh_db_user,\n",
    "        MasterUserPassword = dwh_db_password,\n",
    "        \n",
    "        # Roles (for s3 acces)\n",
    "        IamRoles = [role_arn]\n",
    "    )\n",
    "# If cluster is already created or there is a problem creating it, exception is raised.\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buckle to wait till cluster is created.\n",
    "status = redshift.describe_clusters(ClusterIdentifier = dwh_cluster_identifier)['Clusters'][0]['ClusterStatus']\n",
    "\n",
    "while status == 'creating':\n",
    "    status = redshift.describe_clusters(ClusterIdentifier = dwh_cluster_identifier)['Clusters'][0]['ClusterStatus']\n",
    "    if status != 'creating':\n",
    "        print('Cluster Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>awsuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DBName</td>\n",
       "      <td>pipeline_python_iac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'data-pipeline-python-iac-cluster....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-0bc0d1833e8dfbcb3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Key                                              Value\n",
       "0        NodeType                                          dc2.large\n",
       "1   ClusterStatus                                          available\n",
       "2  MasterUsername                                            awsuser\n",
       "3          DBName                                pipeline_python_iac\n",
       "4        Endpoint  {'Address': 'data-pipeline-python-iac-cluster....\n",
       "5           VpcId                              vpc-0bc0d1833e8dfbcb3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function that takes json values and returns a dataframe, so its easier to read.\n",
    "def pretty_redshift_properties (props):\n",
    "    # Defining keys i want to show.\n",
    "    keys_to_show = ['CusterIdentifier', 'NodeType', 'ClusterStatus', 'MasterUsername', 'DBName', 'Endpoint', 'VpcId']\n",
    "    # Looking for keys in the json items.\n",
    "    x = [(key, value) for key, value in props.items() if key in keys_to_show]\n",
    "    df = pd.DataFrame(data=x, columns=['Key', 'Value'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Getting the json value of cluster properties.    \n",
    "my_cluster_properties = redshift.describe_clusters(ClusterIdentifier = dwh_cluster_identifier)['Clusters'][0]\n",
    "# Passing previous value to my function.\n",
    "pretty_redshift_properties(my_cluster_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all information from the json values, i need all this information in core, to create vpc.\n",
    "dwh_endpoint = my_cluster_properties['Endpoint']['Address']\n",
    "dwh_role_arn = my_cluster_properties['IamRoles'][0]['IamRoleArn']\n",
    "db_name = my_cluster_properties['DBName']\n",
    "db_user = my_cluster_properties['MasterUsername']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-041025bcadc8b904a')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "# Creating security group\n",
    "try:\n",
    "    vpc_id = my_cluster_properties['VpcId']\n",
    "    vpc = ec2.Vpc(id=vpc_id)\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort= int(dwh_port),\n",
    "        ToPort=int(dwh_port)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connectint to database.\n",
    "    conn = psycopg2.connect(host = dwh_endpoint, dbname = db_name, user = db_user, password = dwh_db_password, port = dwh_port)\n",
    "    conn.set_session(autocommit=True)\n",
    "    \n",
    "    # Creating cursor instance.\n",
    "    cur = conn.cursor()\n",
    "except psycopg2.Error as e:\n",
    "    print('Error: Could not connect to postgre database')\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: issue creating table\n",
      "Relation \"users\" already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating data model.\n",
    "try:\n",
    "    create_users_query = ('''\n",
    "                         CREATE TABLE users(\n",
    "                             user_id INT NOT NULL DISTKEY SORTKEY,\n",
    "                             username VARCHAR,\n",
    "                             first_name VARCHAR,\n",
    "                             last_name VARCHAR,\n",
    "                             city VARCHAR,\n",
    "                             state VARCHAR,\n",
    "                             email VARCHAR,\n",
    "                             phone VARCHAR,\n",
    "                             like_sport BOOLEAN,\n",
    "                             like_theater BOOLEAN,\n",
    "                             like_concerts BOOLEAN,\n",
    "                             like_jazz BOOLEAN,\n",
    "                             like_classical BOOLEAN,\n",
    "                             like_opera BOOLEAN,\n",
    "                             like_rock BOOLEAN,\n",
    "                             like_vegas BOOLEAN,\n",
    "                             like_broadway BOOLEAN,\n",
    "                             like_musicales BOOLEAN\n",
    "                        )''')    \n",
    "    cur.execute(create_users_query)\n",
    "    \n",
    "    create_venue_query = ('''\n",
    "                          CREATE TABLE venue(\n",
    "                              venue_id INT NOT NULL DISTKEY SORTKEY,\n",
    "                              venue_name VARCHAR,\n",
    "                              venue_city VARCHAR,\n",
    "                              venue_state VARCHAR,\n",
    "                              venue_seats VARCHAR\n",
    "                        )''')\n",
    "    cur.execute(create_venue_query)\n",
    "    \n",
    "    create_category_query = ('''\n",
    "                             CREATE TABLE category(\n",
    "                                 cat_id INT NOT NULL DISTKEY SORTKEY,\n",
    "                                 cat_group VARCHAR,\n",
    "                                 cat_name VARCHAR,\n",
    "                                 cat_desc VARCHAR\n",
    "                            )''')\n",
    "    cur.execute(create_category_query)\n",
    "    \n",
    "    create_date_query = ('''\n",
    "                        CREATE TABLE date(\n",
    "                            date_id INT NOT NULL DISTKEY SORTKEY,\n",
    "                            cal_date DATE NOT NULL,\n",
    "                            day VARCHAR NoT NULL,\n",
    "                            week INT NOT NULL,\n",
    "                            month VARCHAR NOT NULL,\n",
    "                            qtr VARCHAR NOT NULL,\n",
    "                            year VARCHAR NOT NULL,\n",
    "                            holiday BOOLEAN DEFAULT('N')\n",
    "                        )''')\n",
    "    cur.execute(create_date_query)\n",
    "    \n",
    "    create_event_query = ('''\n",
    "                        CREATE TABLE event(\n",
    "                            event_id INT NOT NULL DISTKEY,\n",
    "                            venue_id INT NOT NULL,\n",
    "                            cat_id INT NOT NULL,\n",
    "                            date_id INT NOT NULL,\n",
    "                            event_name VARCHAR,\n",
    "                            start_time TIMESTAMP\n",
    "                        )''')\n",
    "    cur.execute(create_event_query)\n",
    "    \n",
    "    create_listing_query = ('''\n",
    "                            CREATE TABLE listing(\n",
    "                                list_id INT NOT NULL DISTKEY,\n",
    "                                seller_id INT NOT NULL,\n",
    "                                event_id INT NOT NULL,\n",
    "                                date_id INT NOT NULL,\n",
    "                                num_tickets INT NOT NULL,\n",
    "                                price_per_ticket DECIMAL(8,2),\n",
    "                                total_price DECIMAL(8,2),\n",
    "                                list_time TIMESTAMP\n",
    "                            )''')\n",
    "    cur.execute(create_listing_query)\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print('Error: issue creating table')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>like_sport</th>\n",
       "      <th>like_theater</th>\n",
       "      <th>like_concerts</th>\n",
       "      <th>like_jazz</th>\n",
       "      <th>like_classical</th>\n",
       "      <th>like_opera</th>\n",
       "      <th>like_rock</th>\n",
       "      <th>like_vegas</th>\n",
       "      <th>like_broadway</th>\n",
       "      <th>like_musicales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248807</th>\n",
       "      <td>47735</td>\n",
       "      <td>APM23BQM</td>\n",
       "      <td>Kelsie</td>\n",
       "      <td>Mcclure</td>\n",
       "      <td>Clovis</td>\n",
       "      <td>DC</td>\n",
       "      <td>vitae.dolor@ametfaucibusut.org</td>\n",
       "      <td>(448) 833-7345</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195864</th>\n",
       "      <td>46146</td>\n",
       "      <td>QVK69RZD</td>\n",
       "      <td>Jack</td>\n",
       "      <td>Howell</td>\n",
       "      <td>Troy</td>\n",
       "      <td>PE</td>\n",
       "      <td>eget@sodalesatvelit.edu</td>\n",
       "      <td>(150) 824-8019</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186376</th>\n",
       "      <td>38626</td>\n",
       "      <td>BWH07GDX</td>\n",
       "      <td>Caleb</td>\n",
       "      <td>Burris</td>\n",
       "      <td>Duluth</td>\n",
       "      <td>WA</td>\n",
       "      <td>vitae.velit@faucibusorci.org</td>\n",
       "      <td>(913) 492-3436</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  username first_name last_name    city state  \\\n",
       "248807    47735  APM23BQM     Kelsie   Mcclure  Clovis    DC   \n",
       "195864    46146  QVK69RZD       Jack    Howell    Troy    PE   \n",
       "186376    38626  BWH07GDX      Caleb    Burris  Duluth    WA   \n",
       "\n",
       "                                 email           phone like_sport  \\\n",
       "248807  vitae.dolor@ametfaucibusut.org  (448) 833-7345       True   \n",
       "195864         eget@sodalesatvelit.edu  (150) 824-8019       None   \n",
       "186376    vitae.velit@faucibusorci.org  (913) 492-3436       None   \n",
       "\n",
       "       like_theater like_concerts like_jazz like_classical like_opera  \\\n",
       "248807         None          True      None           True       None   \n",
       "195864         True          None      None           True       True   \n",
       "186376         None          True      None          False       True   \n",
       "\n",
       "       like_rock like_vegas like_broadway like_musicales  \n",
       "248807      None       True          None           None  \n",
       "195864     False       True          None          False  \n",
       "186376      True       None          True          False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I already have my data on S3, so now im going to load it to Redshift.\n",
    "try:\n",
    "    s3_copy_query = ('''\n",
    "                     COPY users from 's3://datapipeline-python-iac/allusers_pipe.txt'\n",
    "                     CREDENTIALS 'aws_iam_role=arn:aws:iam::961251108862:role/redshift-S3-acces'\n",
    "                     delimiter '|'\n",
    "                     region 'sa-east-1'\n",
    "                     ''')\n",
    "    cur.execute(s3_copy_query)\n",
    "    \n",
    "    # Selecting all from table\n",
    "    cur.execute('''SELECT * FROM users''')\n",
    "    # Fetching data\n",
    "    result = cur.fetchall()\n",
    "    \n",
    "    # Getting column names\n",
    "    cols = []\n",
    "    for etl in cur.description:\n",
    "        cols.append(etl[0])\n",
    "    \n",
    "    # Creating Dataframe\n",
    "    df_users = pd.DataFrame(data=result, columns=cols)\n",
    "    \n",
    "except psycopg2.Error as e:\n",
    "    print(e)\n",
    "\n",
    "df_users.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dhdsblend2021')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9689ec58c8207ae0e5df44f3c0da8809222a7a4816f0f8cce8ef27c7a9266816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
